{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "84eddad3",
      "metadata": {
        "id": "84eddad3"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csc_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohsC49cVr-Ox"
      },
      "source": [
        "### Setup and Load dataset"
      ],
      "id": "ohsC49cVr-Ox"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DjzX44HQr-Ox"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from os.path import exists\n",
        "import zipfile\n",
        "import numpy as np"
      ],
      "id": "DjzX44HQr-Ox"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41765245-b418-4b4c-9f49-3150cc606340",
        "id": "woePr-2jr-Ox"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "id": "woePr-2jr-Ox"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGS3UDKhw7TR"
      },
      "id": "dGS3UDKhw7TR",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1gfGS5iL8-8HxdWVtTHXLGWGfPZRvMj6x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258d2f45-bfa3-47c5-948a-469f4f388667",
        "id": "5YlrLKiBr-Oy"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gfGS5iL8-8HxdWVtTHXLGWGfPZRvMj6x\n",
            "To: /content/Music_InCarMusic.zip\n",
            "\r  0% 0.00/152k [00:00<?, ?B/s]\r100% 152k/152k [00:00<00:00, 92.1MB/s]\n"
          ]
        }
      ],
      "id": "5YlrLKiBr-Oy"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"Music_InCarMusic.zip\"  -d  \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncJF1kivt_uR",
        "outputId": "b855ff31-4ffa-4e28-8b68-cbe980de5e9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Music_InCarMusic.zip\n",
            "replace /content/Music_InCarMusic/Data_InCarMusic.xlsx? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/Music_InCarMusic/Data_InCarMusic.xlsx  \n",
            "  inflating: /content/Music_InCarMusic/ReadMe.txt  \n"
          ]
        }
      ],
      "id": "ncJF1kivt_uR"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mtrGQTYOckU",
        "outputId": "9a879aac-55d9-4e43-c407-80cde43e6c84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.8/dist-packages (3.0.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "id": "3mtrGQTYOckU"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "xls = pd.ExcelFile(r\"Music_InCarMusic/Data_InCarMusic.xlsx\") \n",
        "\n",
        "rating_df = xls.parse(0) #2 is the sheet number+1 thus if the file has only 1 sheet write 0 in paranthesis\n",
        "music_df = xls.parse(2)\n",
        "context_df = xls.parse(1)\n",
        "cat_df = xls.parse(3)"
      ],
      "metadata": {
        "id": "vJmVTv85OheC"
      },
      "execution_count": 7,
      "outputs": [],
      "id": "vJmVTv85OheC"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rating_df.rename(columns = {'UserID':'userID'}, inplace = True)\n",
        "rating_df.rename(columns = {'ItemID':'itemID'}, inplace = True)\n",
        "rating_df.rename(columns = {' Rating':'rating'}, inplace = True)\n"
      ],
      "metadata": {
        "id": "iOKGFzEmIZJq"
      },
      "id": "iOKGFzEmIZJq",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR4W7kxwlOmR"
      },
      "source": [
        "## Name of the file where user item interaction data avaiable?\n",
        "- Column name should be in following name and order\n",
        "- [ **itemID, userID , rating , [other features]** ]\n"
      ],
      "id": "xR4W7kxwlOmR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pHemxXOwIqT"
      },
      "source": [
        "### Dense columns"
      ],
      "id": "7pHemxXOwIqT"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "48UYMVFOwMLq"
      },
      "outputs": [],
      "source": [
        "dense_col=None"
      ],
      "id": "48UYMVFOwMLq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn3-WuJWwP9T"
      },
      "source": [
        "### Sparse columns"
      ],
      "id": "Jn3-WuJWwP9T"
    },
    {
      "cell_type": "code",
      "source": [
        "rating_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcHFceqtIhKL",
        "outputId": "b96b3446-3e4a-4ceb-ffc8-01df8f7164da"
      },
      "id": "AcHFceqtIhKL",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['userID', 'itemID', 'rating', 'DrivingStyle', 'landscape', 'mood',\n",
              "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
              "       'weather'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HmKblRwswSQ6"
      },
      "outputs": [],
      "source": [
        "col_sparse= ['DrivingStyle', 'landscape', 'mood',\n",
        "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
        "       'weather']\n"
      ],
      "id": "HmKblRwswSQ6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-n4nO9HvyRq"
      },
      "source": [
        "### User contexual features other than \"user\" column"
      ],
      "id": "b-n4nO9HvyRq"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sIuteHEmwCdU"
      },
      "outputs": [],
      "source": [
        "\n",
        "user_col = ['DrivingStyle', 'landscape', 'mood',\n",
        "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
        "       'weather']\n"
      ],
      "id": "sIuteHEmwCdU"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in user_col:\n",
        "  rating_df[i] = rating_df[i].fillna('NA')"
      ],
      "metadata": {
        "id": "px0D75XvIu-T"
      },
      "id": "px0D75XvIu-T",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_df=rating_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "p3CDyklH1Kv1"
      },
      "id": "p3CDyklH1Kv1",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow recommenders"
      ],
      "metadata": {
        "id": "aHpgrDyYLe9Z"
      },
      "id": "aHpgrDyYLe9Z"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-recommenders==0.6.0\n",
        "\n"
      ],
      "metadata": {
        "id": "DySpFNPLeYvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c67c183-9cc8-4221-8764-fa5156474afb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-recommenders==0.6.0 in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-recommenders==0.6.0) (1.4.0)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-recommenders==0.6.0) (2.9.2)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.12)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.51.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (4.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.30.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.9.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.19.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (15.0.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.2.2)\n"
          ]
        }
      ],
      "id": "DySpFNPLeYvJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs\n",
        "from typing import Dict, Text"
      ],
      "metadata": {
        "id": "etlRCOGlK81S"
      },
      "execution_count": 16,
      "outputs": [],
      "id": "etlRCOGlK81S"
    },
    {
      "cell_type": "code",
      "source": [
        "allcols=['userID','itemID','rating']\n",
        "for i in user_col:\n",
        "  allcols.append(i)\n"
      ],
      "metadata": {
        "id": "-_v6gbS9Js-i"
      },
      "id": "-_v6gbS9Js-i",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in allcols:\n",
        "  rating_df[i]= rating_df[i].astype(str)"
      ],
      "metadata": {
        "id": "obrFee6TJ93Z"
      },
      "id": "obrFee6TJ93Z",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions_dict = rating_df[allcols]\n"
      ],
      "metadata": {
        "id": "cE6nJ0aOXgSs"
      },
      "execution_count": 19,
      "outputs": [],
      "id": "cE6nJ0aOXgSs"
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = tf.data.Dataset.from_tensor_slices(dict(interactions_dict))"
      ],
      "metadata": {
        "id": "WMmIUWfzUY7Y"
      },
      "id": "WMmIUWfzUY7Y",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## item features\n",
        "items_dict = rating_df[['itemID']].drop_duplicates()\n",
        "#items_dict = {name: np.array(value) for name, value in items_dict.items()}\n",
        "items = tf.data.Dataset.from_tensor_slices(dict(items_dict))"
      ],
      "metadata": {
        "id": "rYLVAIf1PDC2"
      },
      "execution_count": 21,
      "outputs": [],
      "id": "rYLVAIf1PDC2"
    },
    {
      "cell_type": "code",
      "source": [
        "itemlists = items.map(lambda x: x['itemID'])"
      ],
      "metadata": {
        "id": "QEeCwFJBTXYE"
      },
      "id": "QEeCwFJBTXYE",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userIds    = rating_df.userID.unique()\n",
        "productIds = rating_df.itemID.unique()\n"
      ],
      "metadata": {
        "id": "758XFYViZpB-"
      },
      "execution_count": 23,
      "outputs": [],
      "id": "758XFYViZpB-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "unique column values"
      ],
      "metadata": {
        "id": "6dy1X6raKJVx"
      },
      "id": "6dy1X6raKJVx"
    },
    {
      "cell_type": "code",
      "source": [
        "unique=dict()\n",
        "for i in allcols:\n",
        " unique[i]= np.unique(np.concatenate(list(ratings.batch(1_000).map(lambda x: x[i]))))"
      ],
      "metadata": {
        "id": "FU7zCE0kKPNj"
      },
      "id": "FU7zCE0kKPNj",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_items = np.unique(np.concatenate(list(items.batch(1000).map(lambda x: x[\"itemID\"]))))"
      ],
      "metadata": {
        "id": "bqeW6KSRUYm8"
      },
      "id": "bqeW6KSRUYm8",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total=ratings.__len__().numpy()\n",
        "train_size=(int) (total * .8 )\n",
        "test_size=(int) (total * .2 )"
      ],
      "metadata": {
        "id": "3DHpM6-VUw8_"
      },
      "execution_count": 26,
      "outputs": [],
      "id": "3DHpM6-VUw8_"
    },
    {
      "cell_type": "code",
      "source": [
        "total_ratings= len(rating_df.index)\n",
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(int(total_ratings), seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take( int(total_ratings*0.8) )\n",
        "test = shuffled.skip(int(total_ratings*0.8)).take(int(total_ratings*0.2))\n",
        "# unique_productIds = unique_items\n",
        "# unique_userIds    = unique_user_ids\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()\n"
      ],
      "metadata": {
        "id": "QXor9fYeUw8_"
      },
      "execution_count": 27,
      "outputs": [],
      "id": "QXor9fYeUw8_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep cross Network"
      ],
      "metadata": {
        "id": "inSByO3A8fA9"
      },
      "id": "inSByO3A8fA9"
    },
    {
      "cell_type": "code",
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding_dimension = 64\n",
        "        self.embedding=dict()\n",
        "        max_tokens = 10_000\n",
        "\n",
        "        ## user id\n",
        "        for col in allcols:\n",
        "          if col=='itemID' or col=='rating':\n",
        "            continue\n",
        "          else:\n",
        "            \n",
        "            self.embedding[col] = tf.keras.Sequential([\n",
        "                                                    tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "                                                    vocabulary=unique[i], mask_token=None),\n",
        "                                                    tf.keras.layers.Embedding(len(unique[col]) + 1, 32),\n",
        "                                                    ])\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        cols=allcols.copy()\n",
        "        cols.remove('itemID')\n",
        "        cols.remove('rating')\n",
        "        # Take the input dictionary, pass it through each input layer,\n",
        "        # and concatenate the result.\n",
        "     \n",
        "        \n",
        "        res=tf.concat([\n",
        "          self.embedding[i](inputs[i])\n",
        "           for i in cols]\n",
        "       \n",
        "        , axis=1)\n",
        "      \n",
        "        return res"
      ],
      "metadata": {
        "id": "4hG7wh_XPx86"
      },
      "id": "4hG7wh_XPx86",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryModel(tf.keras.Model):\n",
        "    \"\"\"Model for encoding user queries.\"\"\"\n",
        "\n",
        "    def __init__(self, layer_sizes, projection_dim=None):\n",
        "        \"\"\"Model for encoding user queries\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # We first use the user model for generating embeddings.\n",
        "        self.embedding_model = UserModel()\n",
        "            \n",
        "\n",
        "        # Then construct the layers.\n",
        "        self.dense_layers = tf.keras.Sequential(tfrs.layers.dcn.Cross(projection_dim=projection_dim,\n",
        "                                        kernel_initializer=\"glorot_uniform\"))\n",
        "\n",
        "        # Use the ReLU activation for all but the last layer.\n",
        "        for layer_size in layer_sizes[:-1]:\n",
        "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "        # No activation for the last layer.\n",
        "        for layer_size in layer_sizes[-1:]:\n",
        "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        feature_embedding = self.embedding_model(inputs)\n",
        "        return self.dense_layers(feature_embedding)"
      ],
      "metadata": {
        "id": "9kPoLJebYhW_"
      },
      "id": "9kPoLJebYhW_",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding_dimension = 64\n",
        "\n",
        "        #max_tokens = 10_000\n",
        "\n",
        "        self.item_embedding = tf.keras.Sequential([\n",
        "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "              vocabulary=unique_items,mask_token=None),\n",
        "          tf.keras.layers.Embedding(len(unique_items) + 1, self.embedding_dimension)\n",
        "        ])\n",
        "\n",
        "        # self.item_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        #     max_tokens=max_tokens)\n",
        "\n",
        "        # self.item_vectorizer.adapt(items)\n",
        "      \n",
        "    def call(self, it):\n",
        "        return tf.concat([\n",
        "            self.item_embedding(it)\n",
        "         \n",
        "        ], axis=1)"
      ],
      "metadata": {
        "id": "HmTiWaD2YlzJ"
      },
      "id": "HmTiWaD2YlzJ",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CandidateModel(tf.keras.Model):\n",
        "    \"\"\"Model for encoding items.\"\"\"\n",
        "\n",
        "    def __init__(self, layer_sizes, projection_dim=None):\n",
        "        \"\"\"Model for encoding items.\n",
        "\n",
        "        \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_model = ItemModel()\n",
        "\n",
        "         # Then construct the layers.\n",
        "        self.dense_layers = tf.keras.Sequential(tfrs.layers.dcn.Cross(projection_dim=projection_dim,\n",
        "                                                kernel_initializer=\"glorot_uniform\"))\n",
        "\n",
        "        # Use the ReLU activation for all but the last layer.\n",
        "        for layer_size in layer_sizes[:-1]:\n",
        "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "        # No activation for the last layer.\n",
        "        for layer_size in layer_sizes[-1:]:\n",
        "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        feature_embedding = self.embedding_model(inputs)\n",
        "        return self.dense_layers(feature_embedding)"
      ],
      "metadata": {
        "id": "dR_Uy7FwRt2y"
      },
      "id": "dR_Uy7FwRt2y",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossDNNModel(tfrs.models.Model):\n",
        "\n",
        "    def __init__(self, layer_sizes,projection_dim=None ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.query_model : tf.keras.Model = QueryModel(layer_sizes)\n",
        "        self.candidate_model : tf.keras.Model = CandidateModel(layer_sizes)\n",
        "        \n",
        "        ## rating and retrieval task.\n",
        "        \n",
        "        self.rating_task = tfrs.tasks.Ranking(\n",
        "            loss=tf.keras.losses.MeanSquaredError(),\n",
        "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "        )\n",
        "                 \n",
        "        self.retrieval_task : tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "            metrics=tfrs.metrics.FactorizedTopK(\n",
        "                candidates=itemlists.batch(128).map(self.candidate_model)\n",
        "            )\n",
        "        )\n",
        "\n",
        "      \n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        \n",
        "        # We only pass the user id and timestamp features into the query model. This\n",
        "        # is to ensure that the training inputs would have the same keys as the\n",
        "        # query inputs. Otherwise the discrepancy in input structure would cause an\n",
        "        # error when loading the query model after saving it.\n",
        "        ratings = features.pop(\"rating\")\n",
        "        cols=allcols.copy()\n",
        "       \n",
        "        cols.remove('itemID')\n",
        "        cols.remove('rating')\n",
        "        query_embeddings = self.query_model({\n",
        "           i: features[i]\n",
        "           for i in cols\n",
        "           \n",
        "        })\n",
        "      \n",
        "        item_embeddings = self.candidate_model(features[\"itemID\"])       \n",
        "        retrieval_loss = self.retrieval_task(query_embeddings, item_embeddings)\n",
        "    \n",
        "    \n",
        "        return self.retrieval_task(query_embeddings, item_embeddings)"
      ],
      "metadata": {
        "id": "9fOA8TGkYxTx"
      },
      "id": "9fOA8TGkYxTx",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cached_train = train.shuffle(train_size).batch(32).cache()\n",
        "cached_test = test.batch(32).cache()\n",
        "\n",
        "model = CrossDNNModel([128,32], \n",
        "                      projection_dim=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "WkMkr2hbY23_"
      },
      "id": "WkMkr2hbY23_",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01))\n",
        "\n",
        "modelhist_dcn=model.fit(cached_train, \n",
        "         epochs=3)\n",
        "\n",
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Retrieval top-50 accuracy: {metrics['factorized_top_k/top_50_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Retrieval top-10 accuracy: {metrics['factorized_top_k/top_10_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Retrieval top-5 accuracy: {metrics['factorized_top_k/top_5_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Retrieval top-1 accuracy: {metrics['factorized_top_k/top_1_categorical_accuracy']:.3f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isj2sVoZAtgS",
        "outputId": "021c5a1f-53c4-4886-afe9-5a4aa3e0380e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "101/101 [==============================] - 15s 116ms/step - root_mean_squared_error: 0.0000e+00 - factorized_top_k/top_1_categorical_accuracy: 0.0153 - factorized_top_k/top_5_categorical_accuracy: 0.0483 - factorized_top_k/top_10_categorical_accuracy: 0.0860 - factorized_top_k/top_50_categorical_accuracy: 0.3739 - factorized_top_k/top_100_categorical_accuracy: 0.7008 - loss: 109.2681 - regularization_loss: 0.0000e+00 - total_loss: 109.2681\n",
            "Epoch 2/3\n",
            "101/101 [==============================] - 12s 123ms/step - root_mean_squared_error: 0.0000e+00 - factorized_top_k/top_1_categorical_accuracy: 0.0081 - factorized_top_k/top_5_categorical_accuracy: 0.0411 - factorized_top_k/top_10_categorical_accuracy: 0.0879 - factorized_top_k/top_50_categorical_accuracy: 0.3668 - factorized_top_k/top_100_categorical_accuracy: 0.7117 - loss: 109.2104 - regularization_loss: 0.0000e+00 - total_loss: 109.2104\n",
            "Epoch 3/3\n",
            "101/101 [==============================] - 10s 101ms/step - root_mean_squared_error: 0.0000e+00 - factorized_top_k/top_1_categorical_accuracy: 0.0103 - factorized_top_k/top_5_categorical_accuracy: 0.0486 - factorized_top_k/top_10_categorical_accuracy: 0.0957 - factorized_top_k/top_50_categorical_accuracy: 0.3951 - factorized_top_k/top_100_categorical_accuracy: 0.7317 - loss: 109.0007 - regularization_loss: 0.0000e+00 - total_loss: 109.0007\n",
            "26/26 [==============================] - 3s 95ms/step - root_mean_squared_error: 0.0000e+00 - factorized_top_k/top_1_categorical_accuracy: 0.0012 - factorized_top_k/top_5_categorical_accuracy: 0.0449 - factorized_top_k/top_10_categorical_accuracy: 0.0860 - factorized_top_k/top_50_categorical_accuracy: 0.3628 - factorized_top_k/top_100_categorical_accuracy: 0.7406 - loss: 102.6955 - regularization_loss: 0.0000e+00 - total_loss: 102.6955\n",
            "Retrieval top-100 accuracy: 0.741.\n",
            "Retrieval top-50 accuracy: 0.363.\n",
            "Retrieval top-10 accuracy: 0.086.\n",
            "Retrieval top-5 accuracy: 0.045.\n",
            "Retrieval top-1 accuracy: 0.001.\n"
          ]
        }
      ],
      "id": "Isj2sVoZAtgS"
    },
    {
      "cell_type": "code",
      "source": [
        "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
        "index.index_from_dataset(\n",
        "  tf.data.Dataset.zip((itemlists.batch(100), itemlists.batch(100).map(model.candidate_model)))\n",
        ")\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFS1JyCP6MOa",
        "outputId": "8a79d449-52ef-4ac6-d164-24b98c6b17a9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7fb23cb90850>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "id": "sFS1JyCP6MOa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "recommendation"
      ],
      "metadata": {
        "id": "mdrQaNhN8grL"
      },
      "id": "mdrQaNhN8grL"
    },
    {
      "cell_type": "code",
      "source": [
        "_, titles = index({\"userID\": np.array(['25']),\n",
        "     \"DrivingStyle\": np.array(['NA']),\n",
        "            \"landscape\":np.array(['NA']),\n",
        "            \"naturalphenomena \" : np.array(['NA']),\n",
        "            \"RoadType\": np.array(['NA']),\n",
        "            \"sleepiness\": np.array(['NA']),\n",
        "            \"trafficConditions\": np.array(['NA']),\n",
        "             \"weather\": np.array(['rainy']),\n",
        "            \"mood\": np.array(['NA']\n",
        "    )},\n",
        "    k=10\n",
        ")\n",
        "titles[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLXiyx5Vlq6S",
        "outputId": "9194cd23-3ac9-4edf-fa33-767abe6c4a91"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "array([b'762', b'751', b'703', b'695', b'249', b'287', b'747', b'755',\n",
              "       b'732', b'710'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "id": "yLXiyx5Vlq6S"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "xR4W7kxwlOmR"
      ]
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}